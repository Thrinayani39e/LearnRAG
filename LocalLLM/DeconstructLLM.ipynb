{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63280afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 76/76 [00:00<00:00, 151.98it/s, Materializing param=transformer.wte.weight]            \n",
      "\u001b[1mGPT2LMHeadModel LOAD REPORT\u001b[0m from: distilgpt2\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "transformer.h.{0, 1, 2, 3, 4, 5}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "Writing model shards: 100%|██████████| 1/1 [00:03<00:00,  3.23s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('downloaded_model/tokenizer_config.json', 'downloaded_model/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from safetensors import safe_open\n",
    "\n",
    "save_directory = \"downloaded_model\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "#downlaod a small model for testing\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9fb38f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the save directory:\n",
      "downloaded_model/tokenizer.json - 3557680 bytes\n",
      "downloaded_model/tokenizer_config.json - 286 bytes\n",
      "downloaded_model/model.safetensors - 327657928 bytes\n",
      "downloaded_model/generation_config.json - 118 bytes\n",
      "downloaded_model/config.json - 1057 bytes\n"
     ]
    }
   ],
   "source": [
    "#print all the files and their sizes in the save_directory\n",
    "print(\"Files in the save directory:\")\n",
    "for root, dirs, files in os.walk(save_directory):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        print(f\"{file_path} - {file_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3739b63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture and hyperparameters in config.json:\n",
      "{\n",
      "    \"_num_labels\": 1,\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": [\n",
      "        \"GPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"dtype\": \"float32\",\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"id2label\": {\n",
      "        \"0\": \"LABEL_0\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"label2id\": {\n",
      "        \"LABEL_0\": 0\n",
      "    },\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"model_type\": \"gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 6,\n",
      "    \"n_positions\": 1024,\n",
      "    \"pad_token_id\": null,\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"task_specific_params\": {\n",
      "        \"text-generation\": {\n",
      "            \"do_sample\": true,\n",
      "            \"max_length\": 50\n",
      "        }\n",
      "    },\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"transformers_version\": \"5.1.0\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#whats in the config.json file interms of the model architecture and hyperparameters?\n",
    "config_path = os.path.join(save_directory, \"config.json\")\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "print(\"Model architecture and hyperparameters in config.json:\")\n",
    "print(json.dumps(config, indent=4))\n",
    "\n",
    "# definitions for the most important parameters in the config.json file\n",
    "# vocab_size: The size of the vocabulary, i.e., the number of unique tokens that the model can understand.\n",
    "# n_positions: The maximum sequence length that the model can process.\n",
    "# n_ctx: The context window size, which is the maximum number of tokens that the model\n",
    "# can attend to at once.\n",
    "# n_embd: The dimensionality of the embeddings, which is the size of the vector\n",
    "# that represents each token in the model.\n",
    "# n_layer: The number of transformer layers in the model.\n",
    "# n_head: The number of attention heads in each transformer layer, which determines\n",
    "# how many different attention patterns the model can learn.\n",
    "# activation_function: The activation function used in the feedforward layers of the model,\n",
    "# which introduces non-linearity into the model's computations.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50eb07ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found weights file: model.safetensors\n",
      "\n",
      "Model contains these weight matrices:\n",
      "Layer Name                                         Shape           Preview\n",
      "--------------------------------------------------------------------------------\n",
      "transformer.h.0.attn.c_attn.bias                   torch.Size([2304]) [0.4693034589290619, -0.4959352910518646, -0.4157843589782715]...\n",
      "transformer.h.0.attn.c_attn.weight                 torch.Size([768, 2304]) [-0.4988037049770355, -0.19897758960723877, -0.1046222522854805]...\n",
      "transformer.h.0.attn.c_proj.bias                   torch.Size([768]) [0.16174378991127014, -0.16444097459316254, -0.15611258149147034]...\n",
      "transformer.h.0.attn.c_proj.weight                 torch.Size([768, 768]) [0.25814932584762573, -0.16598303616046906, 0.062477629631757736]...\n",
      "transformer.h.0.ln_1.bias                          torch.Size([768]) [0.00478767603635788, 0.01292799785733223, -0.018999796360731125]...\n",
      "transformer.h.0.ln_1.weight                        torch.Size([768]) [0.21948328614234924, 0.18534228205680847, 0.15715038776397705]...\n",
      "transformer.h.0.ln_2.bias                          torch.Size([768]) [0.03851698711514473, 0.05805707350373268, 0.013325878418982029]...\n",
      "transformer.h.0.ln_2.weight                        torch.Size([768]) [0.13420014083385468, 0.21758565306663513, 0.20979763567447662]...\n",
      "transformer.h.0.mlp.c_fc.bias                      torch.Size([3072]) [0.04582929611206055, -0.08486124128103256, -0.13325951993465424]...\n",
      "transformer.h.0.mlp.c_fc.weight                    torch.Size([768, 3072]) [0.12394002079963684, 0.10340151190757751, -0.00964804645627737]...\n"
     ]
    }
   ],
   "source": [
    "# Find the weights file\n",
    "weights_file = None\n",
    "for file in files:\n",
    "    if file.endswith(\".bin\") or file.endswith(\".safetensors\"):\n",
    "        weights_file = file\n",
    "        break\n",
    "\n",
    "if weights_file:\n",
    "    print(f\"Found weights file: {weights_file}\")\n",
    "\n",
    "    if weights_file.endswith(\".bin\"):\n",
    "        weights_path = os.path.join(save_directory, weights_file)\n",
    "        state_dict = torch.load(weights_path)\n",
    "\n",
    "        print(\"\\nModel contains these weight matrices:\")\n",
    "        print(f\"{'Layer Name':<50} {'Shape':<15} {'Preview'}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        for i, (name, tensor) in enumerate(list(state_dict.items())[:10]):\n",
    "            # Get first 3 values as preview\n",
    "            preview = tensor.flatten()[:3].tolist()\n",
    "            print(f\"{name:<50} {str(tensor.shape):<15} {preview}...\")\n",
    "\n",
    "    elif weights_file.endswith(\".safetensors\"):\n",
    "        try:\n",
    "            weights_path = os.path.join(save_directory, weights_file)\n",
    "            with safe_open(weights_path, framework=\"pt\") as f:\n",
    "                tensor_names = list(f.keys())[:10]\n",
    "\n",
    "                print(\"\\nModel contains these weight matrices:\")\n",
    "                print(f\"{'Layer Name':<50} {'Shape':<15} {'Preview'}\")\n",
    "                print(\"-\" * 80)\n",
    "\n",
    "                for name in tensor_names:\n",
    "                    tensor = f.get_tensor(name)\n",
    "                    preview = tensor.flatten()[:3].tolist()\n",
    "                    print(f\"{name:<50} {str(tensor.shape):<15} {preview}...\")\n",
    "        except ImportError:\n",
    "            print(\"safetensors library not installed. Run: pip install safetensors\")\n",
    "\n",
    "else:\n",
    "    print(\"No weights file found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a30c33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizer configuration:\n",
      "{\n",
      "    \"add_prefix_space\": false,\n",
      "    \"backend\": \"tokenizers\",\n",
      "    \"bos_token\": \"<|endoftext|>\",\n",
      "    \"eos_token\": \"<|endoftext|>\",\n",
      "    \"errors\": \"replace\",\n",
      "    \"is_local\": false,\n",
      "    \"model_max_length\": 1024,\n",
      "    \"pad_token\": null,\n",
      "    \"tokenizer_class\": \"GPT2Tokenizer\",\n",
      "    \"unk_token\": \"<|endoftext|>\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#tokenizer configuration\n",
    "tokenizer_config_path = os.path.join(save_directory, \"tokenizer_config.json\")\n",
    "with open(tokenizer_config_path, \"r\") as f:\n",
    "    tokenizer_config = json.load(f)\n",
    "print(\"\\nTokenizer configuration:\")\n",
    "print(json.dumps(tokenizer_config, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79605b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizer vocab size: 50257\n",
      "Tokenizer vocabulary (first 10 tokens by id):\n",
      "0: !\n",
      "1: \"\n",
      "2: #\n",
      "3: $\n",
      "4: %\n",
      "5: &\n",
      "6: '\n",
      "7: (\n",
      "8: )\n",
      "9: *\n",
      "\n",
      "Tokenizer vocabulary (sample 10 tokens):\n",
      "[[: 30109\n",
      "ĠPog: 48974\n",
      "Ġinstructed: 17767\n",
      "answer: 41484\n",
      "Ġthreats: 7432\n",
      "ĠWrest: 20722\n",
      "Ġtruck: 7779\n",
      "Ġaliens: 16269\n",
      "ĠInterior: 19614\n",
      "Ġveins: 32375\n"
     ]
    }
   ],
   "source": [
    "# inspect vocab from tokenizer.json via tokenizer API\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(f\"\\nTokenizer vocab size: {len(vocab)}\")\n",
    "\n",
    "# show first 10 tokens by id\n",
    "id_to_token = sorted(vocab.items(), key=lambda item: item[1])\n",
    "print(\"Tokenizer vocabulary (first 10 tokens by id):\")\n",
    "for token, index in id_to_token[:10]:\n",
    "    print(f\"{index}: {token}\")\n",
    "\n",
    "# show a small random sample for sanity\n",
    "print(\"\\nTokenizer vocabulary (sample 10 tokens):\")\n",
    "for token, index in list(vocab.items())[:10]:\n",
    "    print(f\"{token}: {index}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7516c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: unbelievable\n",
      "Tokens: ['un', 'bel', 'iev', 'able']\n",
      "Ids: [403, 6667, 11203, 540]\n",
      "\n",
      "Text: tokenization\n",
      "Tokens: ['token', 'ization']\n",
      "Ids: [30001, 1634]\n",
      "\n",
      "Text: hello world\n",
      "Tokens: ['hello', 'Ġworld']\n",
      "Ids: [31373, 995]\n",
      "\n",
      "Text: running\n",
      "Tokens: ['running']\n",
      "Ids: [20270]\n",
      "\n",
      "Text: RAG pipeline\n",
      "Tokens: ['RAG', 'Ġpipeline']\n",
      "Ids: [33202, 11523]\n"
     ]
    }
   ],
   "source": [
    "# BPE-style tokenization demo using the loaded tokenizer\n",
    "samples = [\"unbelievable\", \"tokenization\", \"hello world\", \"running\", \"RAG pipeline\"]\n",
    "for text in samples:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Ids: {ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb595d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample text: The quick brown fox jumps over the lazy dog.\n",
      "Tokens: ['The', 'Ġquick', 'Ġbrown', 'Ġfox', 'Ġjumps', 'Ġover', 'Ġthe', 'Ġlazy', 'Ġdog', '.']\n",
      "Ids: [464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290, 13]\n",
      "BPE Tokens: ['The', 'Ġquick', 'Ġbrown', 'Ġfox', 'Ġjumps', 'Ġover', 'Ġthe', 'Ġlazy', 'Ġdog', '.']\n"
     ]
    }
   ],
   "source": [
    "#will see how tokenization works for a sample text and how it maps to the model's input format\n",
    "sample_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"\\nSample text: {sample_text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Ids: {ids}\")\n",
    "# how is it merging subwords into words? let's see the mapping of tokens to original text through bpe tokenization\n",
    "bpe_tokens = tokenizer(sample_text, return_tensors=\"pt\")[\"input_ids\"][0].tolist()\n",
    "bpe_tokens_str = [tokenizer.convert_ids_to_tokens([id])[0] for id in bpe_tokens]\n",
    "print(f\"BPE Tokens: {bpe_tokens_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01cddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the files in the save_directory and whats their purpose?\n",
    "# config.json - contains the model architecture and hyperparameters\n",
    "# pytorch_model.bin - contains the model weights (if using PyTorch)\n",
    "# safetensors - contains the model weights in a more efficient format (if using safetensors), \n",
    "# we dont have pytorch_model.bin but we have safetensors\n",
    "# tokenizer_config.json - contains the tokenizer configuration and vocabulary information\n",
    "# generation_config.json - contains the default generation parameters for the model (if present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212d283f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
