{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c8cecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.10.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (31 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.25.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: filelock in /workspaces/LearnRAG/.venv/lib/python3.12/site-packages (from torch) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /workspaces/LearnRAG/.venv/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Collecting setuptools (from torch)\n",
      "  Downloading setuptools-81.0.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Downloading networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /workspaces/LearnRAG/.venv/lib/python3.12/site-packages (from torch) (2026.2.0)\n",
      "Collecting cuda-bindings==12.9.4 (from torch)\n",
      "  Downloading cuda_bindings-12.9.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.6 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.4.5 (from torch)\n",
      "  Downloading nvidia_nvshmem_cu12-3.4.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.6.0 (from torch)\n",
      "  Downloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting cuda-pathfinder~=1.1 (from cuda-bindings==12.9.4->torch)\n",
      "  Downloading cuda_pathfinder-1.3.3-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: numpy in /workspaces/LearnRAG/.venv/lib/python3.12/site-packages (from torchvision) (2.4.2)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Downloading pillow-12.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
      "Downloading torch-2.10.0-cp312-cp312-manylinux_2_28_x86_64.whl (915.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m915.7/915.7 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m  \u001b[33m0:00:24\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading cuda_bindings-12.9.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m  \u001b[33m0:00:14\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m  \u001b[33m0:00:15\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.4.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (139.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (188.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading cuda_pathfinder-1.3.3-py3-none-any.whl (27 kB)\n",
      "Downloading torchvision-0.25.0-cp312-cp312-manylinux_2_28_x86_64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-12.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
      "Downloading setuptools-81.0.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, setuptools, pillow, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, cuda-pathfinder, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, cuda-bindings, nvidia-cusolver-cu12, torch, torchvision\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27/27\u001b[0m [torchvision]\u001b[0m [torchvision]ver-cu12]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 cuda-bindings-12.9.4 cuda-pathfinder-1.3.3 jinja2-3.1.6 mpmath-1.3.0 networkx-3.6.1 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.4.5 nvidia-nvtx-cu12-12.8.90 pillow-12.1.0 setuptools-81.0.0 sympy-1.14.0 torch-2.10.0 torchvision-0.25.0 triton-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e0f1d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 76/76 [00:00<00:00, 593.96it/s, Materializing param=transformer.wte.weight]            \n",
      "\u001b[1mGPT2LMHeadModel LOAD REPORT\u001b[0m from: distilgpt2\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "transformer.h.{0, 1, 2, 3, 4, 5}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#lets run a full llm pipeline\n",
    "# Input text → Tokenization → Converting to IDs → Model processing → Next token prediction → Token selection → Building the response\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "save_directory = \"./downloaded_model\"  # Change this to your preferred path\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06788e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "Loading weights: 100%|██████████| 76/76 [00:00<00:00, 312.94it/s, Materializing param=transformer.wte.weight]            \n"
     ]
    }
   ],
   "source": [
    "#load model and tokenizer from the saved directory\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model.save_pretrained(save_directory)\n",
    "# Load the model and tokenizer from the saved directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "model = AutoModelForCausalLM.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb721341",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt or input\n",
    "prompt = \"What is the capital of France?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3f1f375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization Result:\n",
      "Token 1: 'What'\n",
      "Token 2: 'Ġis'\n",
      "Token 3: 'Ġthe'\n",
      "Token 4: 'Ġcapital'\n",
      "Token 5: 'Ġof'\n",
      "Token 6: 'ĠFrance'\n",
      "Token 7: '?'\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input\n",
    "tokens = tokenizer.tokenize(prompt)\n",
    "\n",
    "print(\"Tokenization Result:\")\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"Token {i+1}: '{token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d66a1b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input IDs: [2061, 318, 262, 3139, 286, 4881, 30]\n",
      "\n",
      "Input Tensor Shape: torch.Size([1, 7])\n",
      "Input Tensor Type: torch.int64\n"
     ]
    }
   ],
   "source": [
    "# convert tokens to input IDs\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"\\nInput IDs:\", input_ids)\n",
    "#model input tensor shape and type\n",
    "import torch\n",
    "input_tensor = torch.tensor([input_ids])\n",
    "print(\"\\nInput Tensor Shape:\", input_tensor.shape)\n",
    "print(\"Input Tensor Type:\", input_tensor.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33949126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Next Token Logits Shape: torch.Size([1, 7, 50257])\n",
      "\n",
      "Next Token Logits Shape: torch.Size([7, 50257])\n",
      "Next Token Logits Type: torch.float32\n"
     ]
    }
   ],
   "source": [
    "#model processing \n",
    "with torch.no_grad():\n",
    "    outputs = model(input_tensor)\n",
    "    next_token_logits = outputs.logits\n",
    "\n",
    "#output logits shape and type\n",
    "print(\"\\nNext Token Logits Shape:\", next_token_logits.shape)\n",
    "print(\"\\nNext Token Logits Shape:\", next_token_logits[0].shape)\n",
    "print(\"Next Token Logits Type:\", next_token_logits.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d5dfb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Predictions for Next Token:\n",
      "----------------------------------------\n",
      "Token           ID       Probability\n",
      "----------------------------------------\n",
      "'\\n'            198      23.75%\n",
      "'�'             447      6.55%\n",
      "' It'           632      4.33%\n",
      "' The'          383      4.09%\n",
      "' I'            314      2.51%\n",
      "' And'          843      2.51%\n",
      "' What'         1867     2.28%\n",
      "' Is'           1148     2.28%\n",
      "' A'            317      1.47%\n",
      "' In'           554      1.39%\n"
     ]
    }
   ],
   "source": [
    "# We want the predictions for the last position (after \"transforming\")\n",
    "next_token_logits = next_token_logits[0, -1, :]\n",
    "\n",
    "# Convert logits to probabilities\n",
    "next_token_probs = torch.softmax(next_token_logits, dim=0)\n",
    "\n",
    "# Get the top 10 most likely tokens\n",
    "top_k = 10\n",
    "topk_probs, topk_indices = torch.topk(next_token_probs, top_k)\n",
    "\n",
    "# Convert to lists for easier handling\n",
    "topk_probs = topk_probs.detach().numpy()\n",
    "topk_indices = topk_indices.detach().numpy()\n",
    "\n",
    "# Get the corresponding tokens\n",
    "topk_tokens = [tokenizer.decode([idx]) for idx in topk_indices]\n",
    "\n",
    "print(\"Top 10 Predictions for Next Token:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Token':<15} {'ID':<8} {'Probability':<10}\")\n",
    "print(\"-\" * 40)\n",
    "for i in range(top_k):\n",
    "    print(f\"{repr(topk_tokens[i]):<15} {topk_indices[i]:<8} {topk_probs[i]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f184430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Greedy Predicted Next Token: '\n",
      "' (ID:198)\n",
      "\n",
      "Token Selection Candidates (Top-k):\n",
      "----------------------------------------\n",
      "Token           ID       Probability Selected\n",
      "----------------------------------------\n",
      "'\\n'            198      23.75% *\n",
      "'�'             447      6.55% \n",
      "' It'           632      4.33% \n",
      "' The'          383      4.09% \n",
      "' I'            314      2.51% \n",
      "' And'          843      2.51% \n",
      "' What'         1867     2.28% \n",
      "' Is'           1148     2.28% \n",
      "' A'            317      1.47% \n",
      "' In'           554      1.39% \n"
     ]
    }
   ],
   "source": [
    "#token selection (greedy)\n",
    "predicted_token_id = torch.argmax(next_token_logits).item()\n",
    "predicted_token = tokenizer.decode([predicted_token_id])\n",
    "print(f\"\\nGreedy Predicted Next Token: '{predicted_token}' (ID:{predicted_token_id})\")\n",
    "\n",
    "# display token-selection candidates (top-k) and highlight the chosen token\n",
    "print(\"\\nToken Selection Candidates (Top-k):\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Token':<15} {'ID':<8} {'Probability':<10} {'Selected'}\")\n",
    "print(\"-\" * 40)\n",
    "for i in range(top_k):\n",
    "    is_selected = \"*\" if int(topk_indices[i]) == predicted_token_id else \"\"\n",
    "    print(f\"{repr(topk_tokens[i]):<15} {topk_indices[i]:<8} {topk_probs[i]*100:.2f}% {is_selected}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4dcee865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Response (Greedy): 'What is the capital of France?\n",
      "'\n",
      "\n",
      "Possible Responses (Top-k):\n",
      " 1. What is the capital of France?\n",
      "\n",
      " 2. What is the capital of France?�\n",
      " 3. What is the capital of France? It\n",
      " 4. What is the capital of France? The\n",
      " 5. What is the capital of France? I\n",
      " 6. What is the capital of France? And\n",
      " 7. What is the capital of France? What\n",
      " 8. What is the capital of France? Is\n",
      " 9. What is the capital of France? A\n",
      "10. What is the capital of France? In\n"
     ]
    }
   ],
   "source": [
    "#building the response (greedy)\n",
    "response = prompt + predicted_token\n",
    "print(f\"\\nGenerated Response (Greedy): '{response}'\")\n",
    "\n",
    "# show many possible responses from top-k next-token candidates\n",
    "print(\"\\nPossible Responses (Top-k):\")\n",
    "for i in range(top_k):\n",
    "    candidate_response = prompt + topk_tokens[i]\n",
    "    print(f\"{i+1:>2}. {candidate_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f05fd0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prompt: 'AI is Transforming'\n",
      "\n",
      "--- Step 1: Generating token #4 ---\n",
      "\n",
      "Top candidates:\n",
      "  1. ' the' (ID: 262, Probability: 74.38%)\n",
      "  2. ' a' (ID: 257, Probability: 2.00%)\n",
      "  3. ' The' (ID: 383, Probability: 1.61%)\n",
      "  4. ' and' (ID: 290, Probability: 1.54%)\n",
      "  5. '.' (ID: 13, Probability: 1.38%)\n",
      "\n",
      "Selected token: ' the'\n",
      "Selected token raw: 'Ġthe'\n",
      "Selected token repr: ' the'\n",
      "Text so far: 'AI is Transforming the'\n",
      "Text so far repr: 'AI is Transforming the'\n",
      "\n",
      "--- Step 2: Generating token #5 ---\n",
      "\n",
      "Top candidates:\n",
      "  1. ' World' (ID: 2159, Probability: 10.41%)\n",
      "  2. ' Internet' (ID: 4455, Probability: 3.91%)\n",
      "  3. ' world' (ID: 995, Probability: 3.34%)\n",
      "  4. ' U' (ID: 471, Probability: 3.12%)\n",
      "  5. ' US' (ID: 1294, Probability: 2.03%)\n",
      "\n",
      "Selected token: ' Internet'\n",
      "Selected token raw: 'ĠInternet'\n",
      "Selected token repr: ' Internet'\n",
      "Text so far: 'AI is Transforming the Internet'\n",
      "Text so far repr: 'AI is Transforming the Internet'\n",
      "\n",
      "--- Step 3: Generating token #6 ---\n",
      "\n",
      "Top candidates:\n",
      "  1. '.' (ID: 13, Probability: 23.24%)\n",
      "  2. ',' (ID: 11, Probability: 11.62%)\n",
      "  3. ' to' (ID: 284, Probability: 10.19%)\n",
      "  4. '\n",
      "' (ID: 198, Probability: 10.11%)\n",
      "  5. ' and' (ID: 290, Probability: 8.17%)\n",
      "\n",
      "Selected token: '.'\n",
      "Selected token raw: '.'\n",
      "Selected token repr: '.'\n",
      "Text so far: 'AI is Transforming the Internet.'\n",
      "Text so far repr: 'AI is Transforming the Internet.'\n",
      "\n",
      "--- Step 4: Generating token #7 ---\n",
      "\n",
      "Top candidates:\n",
      "  1. '\n",
      "' (ID: 198, Probability: 33.12%)\n",
      "  2. ' It' (ID: 632, Probability: 23.50%)\n",
      "  3. ' The' (ID: 383, Probability: 10.42%)\n",
      "  4. '�' (ID: 447, Probability: 6.08%)\n",
      "  5. ' We' (ID: 775, Probability: 5.94%)\n",
      "\n",
      "Selected token: ' It'\n",
      "Selected token raw: 'ĠIt'\n",
      "Selected token repr: ' It'\n",
      "Text so far: 'AI is Transforming the Internet. It'\n",
      "Text so far repr: 'AI is Transforming the Internet. It'\n",
      "\n",
      "--- Step 5: Generating token #8 ---\n",
      "\n",
      "Top candidates:\n",
      "  1. '�' (ID: 447, Probability: 37.55%)\n",
      "  2. ' is' (ID: 318, Probability: 33.80%)\n",
      "  3. ''s' (ID: 338, Probability: 11.37%)\n",
      "  4. ' will' (ID: 481, Probability: 6.60%)\n",
      "  5. ' has' (ID: 468, Probability: 2.70%)\n",
      "\n",
      "Selected token: ' is'\n",
      "Selected token raw: 'Ġis'\n",
      "Selected token repr: ' is'\n",
      "Text so far: 'AI is Transforming the Internet. It is'\n",
      "Text so far repr: 'AI is Transforming the Internet. It is'\n",
      "\n",
      "Final generated text: 'AI is Transforming the Internet. It is'\n"
     ]
    }
   ],
   "source": [
    "#step by step llm pipeline demonstration complete!\n",
    "import numpy as np\n",
    "\n",
    "def generate_step_by_step(prompt, max_new_tokens=5, temperature=0.7, top_k=5):\n",
    "    \"\"\"Generate text token by token with detailed output at each step\"\"\"\n",
    "    # Start with the prompt\n",
    "    current_text = prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    print(f\"Starting prompt: '{prompt}'\\n\")\n",
    "    \n",
    "    # Generate new tokens one by one\n",
    "    for i in range(max_new_tokens):\n",
    "        print(f\"--- Step {i+1}: Generating token #{len(prompt.split())+i+1} ---\")\n",
    "        \n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "        \n",
    "        # Get next token logits (predictions for the next token)\n",
    "        next_token_logits = outputs.logits[0, -1, :]\n",
    "        \n",
    "        # Apply temperature\n",
    "        next_token_logits = next_token_logits / temperature\n",
    "        \n",
    "        # Get top-k token indices and their probabilities\n",
    "        topk_probs, topk_indices = torch.topk(torch.softmax(next_token_logits, dim=0), top_k)\n",
    "        \n",
    "        # Print the top candidates\n",
    "        print(\"\\nTop candidates:\")\n",
    "        for j in range(top_k):\n",
    "            token_id = topk_indices[j].item()\n",
    "            token_text = tokenizer.decode([token_id])\n",
    "            token_prob = topk_probs[j].item() * 100\n",
    "            print(f\"  {j+1}. '{token_text}' (ID: {token_id}, Probability: {token_prob:.2f}%)\")\n",
    "        \n",
    "        # Renormalize probabilities for top-k\n",
    "        topk_probs = topk_probs / topk_probs.sum()\n",
    "        \n",
    "        # Sample from top-k\n",
    "        chosen_idx = np.random.choice(topk_indices.detach().numpy(), p=topk_probs.detach().numpy())\n",
    "        chosen_token = tokenizer.decode([chosen_idx])\n",
    "        chosen_token_raw = tokenizer.convert_ids_to_tokens([int(chosen_idx)])[0]\n",
    "        \n",
    "        print(f\"\\nSelected token: '{chosen_token}'\")\n",
    "        print(f\"Selected token raw: {chosen_token_raw!r}\")\n",
    "        print(f\"Selected token repr: {chosen_token!r}\")\n",
    "        \n",
    "        # Update for next iteration\n",
    "        next_token = torch.tensor([[chosen_idx]])\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        current_text += chosen_token\n",
    "        \n",
    "        print(f\"Text so far: '{current_text}'\")\n",
    "        print(f\"Text so far repr: {current_text!r}\\n\")\n",
    "    \n",
    "    print(f\"Final generated text: '{current_text}'\")\n",
    "    return current_text\n",
    "\n",
    "# Generate text step by step\n",
    "prompt = \"AI is Transforming\"\n",
    "final_text = generate_step_by_step(prompt, max_new_tokens=5, temperature=0.7, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "862f5eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect of Generation Parameters:\n",
      "\n",
      "Greedy (no sampling)\n",
      "Parameters: {'do_sample': False}\n",
      "Input: AI is Transforming\n",
      "Generated:  the World.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Low Temperature (0.3)\n",
      "Parameters: {'temperature': 0.3, 'do_sample': True}\n",
      "Input: AI is Transforming\n",
      "Generated:  the world, and the world is changing.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "High Temperature (1.5)\n",
      "Parameters: {'temperature': 1.5, 'do_sample': True}\n",
      "Input: AI is Transforming\n",
      "Generated:  the 'Reduce' as per a model\n",
      "\n",
      "\n",
      "Categorized\n",
      "--------------------------------------------------------------------------------\n",
      "Top-k (5)\n",
      "Parameters: {'top_k': 5, 'do_sample': True}\n",
      "Input: AI is Transforming\n",
      "Generated:  the world.››\n",
      "The world is not the same as\n",
      "--------------------------------------------------------------------------------\n",
      "Top-p (0.9)\n",
      "Parameters: {'top_p': 0.9, 'do_sample': True}\n",
      "Input: AI is Transforming\n",
      "Generated:  the European Space Agency. (1) This paper examines the feasibility of integrating\n",
      "--------------------------------------------------------------------------------\n",
      "Balanced\n",
      "Parameters: {'temperature': 0.7, 'top_k': 50, 'top_p': 0.9, 'do_sample': True}\n",
      "Input: AI is Transforming\n",
      "Generated:  the World by Designing a New World.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to generate text with different parameters\n",
    "def generate_with_params(prompt, max_new_tokens=15, **params):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Set up generation parameters\n",
    "    gen_params = {}\n",
    "    if 'temperature' in params:\n",
    "        gen_params['temperature'] = params['temperature']\n",
    "    if 'top_k' in params:\n",
    "        gen_params['top_k'] = params['top_k']\n",
    "    if 'top_p' in params:\n",
    "        gen_params['top_p'] = params['top_p']\n",
    "    if 'do_sample' in params:\n",
    "        gen_params['do_sample'] = params['do_sample']\n",
    "    \n",
    "    # Generate the output\n",
    "    output_ids = model.generate(\n",
    "        input_ids, \n",
    "        max_length=len(input_ids[0]) + max_new_tokens,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        **gen_params\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Try different parameter combinations\n",
    "params_to_try = [\n",
    "    {'name': 'Greedy (no sampling)', 'params': {'do_sample': False}},\n",
    "    {'name': 'Low Temperature (0.3)', 'params': {'temperature': 0.3, 'do_sample': True}},\n",
    "    {'name': 'High Temperature (1.5)', 'params': {'temperature': 1.5, 'do_sample': True}},\n",
    "    {'name': 'Top-k (5)', 'params': {'top_k': 5, 'do_sample': True}},\n",
    "    {'name': 'Top-p (0.9)', 'params': {'top_p': 0.9, 'do_sample': True}},\n",
    "    {'name': 'Balanced', 'params': {'temperature': 0.7, 'top_k': 50, 'top_p': 0.9, 'do_sample': True}}\n",
    "]\n",
    "\n",
    "# Generate and display results\n",
    "print(\"Effect of Generation Parameters:\\n\")\n",
    "\n",
    "for setting in params_to_try:\n",
    "    output = generate_with_params(prompt, **setting['params'])\n",
    "    generated_part = output[len(prompt):]\n",
    "    \n",
    "    print(f\"{setting['name']}\")\n",
    "    print(f\"Parameters: {setting['params']}\")\n",
    "    print(f\"Input: {prompt}\")\n",
    "    print(f\"Generated: {generated_part}\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
